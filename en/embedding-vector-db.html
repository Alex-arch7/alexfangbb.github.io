<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-colors-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="built-on" content="2024-02-22T15:48:56.153685"><title>Input Embedding | LLM: From Zero to Hero</title><script type="application/json" id="virtual-toc-data">[{"id":"word-vector-embeddings","level":0,"title":"Word Vector Embeddings","anchor":"#word-vector-embeddings"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b205/app.css" rel="stylesheet"><link rel="icon" type="image/png" sizes="16x16" href="images/cropped_logo.png"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="Input Embedding | LLM: From Zero to Hero"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="LLM: From Zero to Hero Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="embedding-vector-db.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="Input Embedding | LLM: From Zero to Hero"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "embedding-vector-db.html#webpage",
    "url": "embedding-vector-db.html",
    "name": "Input Embedding | LLM: From Zero to Hero",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "/#website",
    "url": "/",
    "name": "LLM: From Zero to Hero Help"
}</script><!-- End Schema.org --></head><body data-id="Embedding-Vector-DB" data-main-title="Input Embedding" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs="Advanced-Topics.md|Advanced Topics"><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>LLM: From Zero to Hero  Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="Embedding-Vector-DB" id="Embedding-Vector-DB.md">Input Embedding</h1><p id="8ac23da_11">To be updated...</p><section class="chapter"><h2 id="word-vector-embeddings" data-toc="word-vector-embeddings">Word Vector Embeddings</h2><p id="8ac23da_12">A conceptual understanding of word vector embeddings is pretty much fundamental to understanding natural language processing. In essence, a word vector embedding takes individual words and translates them into a vector which somehow represents its meaning.</p><p id="8ac23da_13">The details can vary from implementation to implementation, but the end result can be thought of as a &ldquo;space of words&rdquo;, where the space obeys certain convenient relationships. Words are hard to do math on, but vectors which contain information about a word, and how they relate to other words, are significantly easier to do math on. This task of converting words to vectors is often referred to as an &ldquo;embedding&rdquo;.</p><p id="8ac23da_14">The most common way to generate word vector embeddings is to use a neural network. The neural network is trained on a large corpus of text, and learns to predict the surrounding words of a given word. The weights of the neural network are then used as the word vector embeddings.</p><p id="8ac23da_15">As the state of the art has progressed, word embeddings have maintained an important tool, with GloVe, Word2Vec, and FastText all being popular choices. Sub-word embeddings are generally much more powerful than full word embeddings.</p><p id="8ac23da_16">The Landmark Paper, <a href="https://arxiv.org/abs/1409.0473" id="8ac23da_17" data-external="true" rel="noopener noreferrer">Neural Machine Translation by Jointly Learning to Align and Translate</a> popularized the general concept of attention and was the conceptual precursor to the multi-headed self attention mechanisms used in transformers.</p><p id="8ac23da_18">In other words, it decides which inputs are currently relevant, and which inputs are not currently relevant.</p></section><div class="last-modified">Last modified: 22 February 2024</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="tokenization.html" class="navigation-links__prev">Tokenization</a><a href="positional-encoding.html" class="navigation-links__next">Positional Encoding</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b205/app.js"></script></body></html>