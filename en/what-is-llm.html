<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-colors-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="built-on" content="2024-02-22T15:48:56.171742"><title>What is LLM | LLM: From Zero to Hero</title><script type="application/json" id="virtual-toc-data">[{"id":"model-definition","level":0,"title":"Model Definition","anchor":"#model-definition"},{"id":"model-is-just-a-file","level":0,"title":"Model is just a file","anchor":"#model-is-just-a-file"},{"id":"model-inference","level":0,"title":"Model Inference","anchor":"#model-inference"},{"id":"model-architecture","level":0,"title":"Model Architecture","anchor":"#model-architecture"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b205/app.css" rel="stylesheet"><link rel="icon" type="image/png" sizes="16x16" href="images/cropped_logo.png"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="What is LLM | LLM: From Zero to Hero"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="LLM: From Zero to Hero Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="what-is-llm.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="What is LLM | LLM: From Zero to Hero"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "what-is-llm.html#webpage",
    "url": "what-is-llm.html",
    "name": "What is LLM | LLM: From Zero to Hero",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "/#website",
    "url": "/",
    "name": "LLM: From Zero to Hero Help"
}</script><!-- End Schema.org --></head><body data-id="What-is-LLM" data-main-title="What is LLM" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs=""><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>LLM: From Zero to Hero  Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="What-is-LLM" id="What-is-LLM.md">What is LLM</h1><section class="chapter"><h2 id="model-definition" data-toc="model-definition">Model Definition</h2><p id="e4b49296_73">Large language models (LLMs) like ChatGPT are taking the tech world today. From <a href="https://en.wikipedia.org/wiki/Large_language_model" id="e4b49296_74" data-external="true" rel="noopener noreferrer">Wikipedia</a>, the definition of LLM is:</p><aside class="prompt" data-type="tip" data-title="" id="e4b49296_75"><p id="e4b49296_76">A large language model (LLM) is a language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks following a transformer architecture.</p></aside><p id="e4b49296_77">in other words:</p><p id="e4b49296_78"><span class="control" id="e4b49296_79">LLMs are trained on vast datasets of texts like books, websites or user generated contents. They can generate new text that continues an initial prompt in a natural way.</span></p><p id="e4b49296_80">LLM model is basically a neural network with a lot of parameters. Roughly speaking, the more parameters, the better the model is. So we always hear about the size of the model, which is the number of parameters. For example, GPT-3 has 175 billion parameters, and GPT-4 may have over 1 trillion parameters.</p><p id="e4b49296_81"><span class="control" id="e4b49296_82">But what exactly a model looks like?</span></p></section><section class="chapter"><h2 id="model-is-just-a-file" data-toc="model-is-just-a-file">Model is just a file</h2><p id="e4b49296_83">A language model is just a binary file:</p><figure id="e4b49296_84"><img alt="A language model is just a file" src="images/a-language-model-is-just-a-file.png" title="A language model is just a file" width="400" height="465"></figure><p id="e4b49296_85">(from <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" id="e4b49296_86" data-external="true" rel="noopener noreferrer">Andrej Karpathy's build a GPT model from scratch)</a>)</p><p id="e4b49296_87">In above image, the <span class="emphasis" id="e4b49296_88"><span class="control" id="e4b49296_89">parameters</span></span> file is <span class="emphasis" id="e4b49296_90">Meta's Llama-2-70b <span class="control" id="e4b49296_91">model</span></span>, and its size is 140GB which contains 70b parameters (in a format of digits). The <span class="emphasis" id="e4b49296_92">run.c</span> file is the inference program, which is used to query the model.</p><p id="e4b49296_93">Training these super large models are very expensive. It costs millions of dollars to train a model like GPT-3.</p><figure id="e4b49296_94"><img alt="Train on gpu concept" src="images/train-on-gpu-concept.png" title="Train on gpu concept" width="600" height="303"></figure><p id="e4b49296_95">(from <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" id="e4b49296_96" data-external="true" rel="noopener noreferrer">Andrej Karpathy's build a GPT model from scratch)</a>)</p><p id="e4b49296_97">As of today, the most outstanding model GPT-4 is no longer a single model, but a mixture of several models. Each model is trained or fine-tuned on a specific domain and work together to bring the best perform during inference.</p><p id="e4b49296_98">But don't worry, our goal is to understand and learn about the fundamentals of large language models. Fortunately, we can still train a model (with much fewer parameters) on our personal computers. We will dive in and code it together, step-by-step, in the <a href="let-s-code-llm.html" id="e4b49296_99" data-tooltip="The full structured code in this article can be downloaded from github.com/waylandzhang/Transformer-from-scratch.">Let's Code LLM</a> section soon.</p></section><section class="chapter"><h2 id="model-inference" data-toc="model-inference">Model Inference</h2><p id="e4b49296_100">When the model is trained and ready, a user queries the model with a question, the question text is passed into that 140GB file and processed character-by-character then return the <span class="emphasis" id="e4b49296_101">most relevant</span> text as result outputs.</p><p id="e4b49296_102">By meaning the <span class="control" id="e4b49296_103">most relevant</span>, it means the model will return the text that are most likely to be the next character of the input text.</p><p id="e4b49296_104">For example,</p><div class="code-block" data-lang="none">
&gt; Input: &quot;I like to eat&quot;

&gt; Output: &quot;apple&quot;

</div><p id="e4b49296_106"><code class="code" id="e4b49296_107">apple</code> is returned as the next character, because &quot;apple&quot; is the most likely next character of &quot;I like to eat&quot; - based on the large datasets which the model was trained on.</p><p id="e4b49296_108">Remember we mentioned books, websites above? Base on the chunk of data we provide, you can think of this way:</p><p id="e4b49296_109"><code class="code" id="e4b49296_110">I like to eat apple</code> is a very common sentence that the model <span class="emphasis" id="e4b49296_111">learned</span> for multiple times.</p><p id="e4b49296_112"><code class="code" id="e4b49296_113">I like to eat banana</code> is also a common sentence but less common than the one above.</p><p id="e4b49296_114">So during training, the model:</p><aside class="prompt" data-type="note" data-title="" id="e4b49296_115"><p id="e4b49296_116">recorded <code class="code" id="e4b49296_117">apple</code> with <span class="control" id="e4b49296_118">0.375</span> probability to appear after <code class="code" id="e4b49296_119">I like to eat</code></p><p id="e4b49296_120">recorded <code class="code" id="e4b49296_121">banana</code> with <span class="control" id="e4b49296_122">0.146</span> probability to appear after <code class="code" id="e4b49296_123">I like to eat</code></p><p id="e4b49296_124"><span class="emphasis" id="e4b49296_125">... other character probabilities ...</span></p></aside><p id="e4b49296_126">So these probabilities result in <span class="emphasis" id="e4b49296_127">probability sets</span> saved in the <span class="emphasis" id="e4b49296_128"><span class="control" id="e4b49296_129">parameters</span></span> model file. (<span class="emphasis" id="e4b49296_130">Probabilities</span> usually called <span class="emphasis" id="e4b49296_131">Weights</span> in machine learning field.)</p><p id="e4b49296_132"><span class="emphasis" id="e4b49296_133">So basically, an LLM model is a probabilistic database that assigns a probability distribution to any given character and its relevant contextual characters.</span></p><p id="e4b49296_134">This sounds impossible before. But since the paper <a href="https://arxiv.org/abs/2005.14165" id="e4b49296_135" data-external="true" rel="noopener noreferrer">《Attention is all you need》</a> was published in 2017, the <span class="control" id="e4b49296_136">transformer architecture</span> was introduced to enable such contextual understanding possible by training a neural network on a very large dataset.</p></section><section class="chapter"><h2 id="model-architecture" data-toc="model-architecture">Model Architecture</h2><p id="e4b49296_137">Before the advent of the LLM, machine training on neural networks was indeed limited to using relatively small datasets and had minimal capacity for contextual understanding. This means that early models were not able to understand text in the same way that humans do.</p><p id="e4b49296_138">When the paper was first published, it was intended for training language-translation purpose models. However, the team at OpenAI discovered that the transformer architecture was the crucial solution for character prediction. Once trained on the entirety of internet data, the model could potentially comprehend the context of any text and coherently complete any sentence, much like a human.</p><p id="e4b49296_139">Below is a diagram showing what happens inside the model training process:</p><figure id="e4b49296_140"><img alt="Llm diagram1" src="images/llm-diagram1.png" title="Llm diagram1" width="800" height="411"></figure><p id="e4b49296_141">Of-course we don't understand that at the first time we see it, but don't worry we will soon walk through it in the following topics.</p><p id="e4b49296_142">Before we delve into the programmatic aspects and mathematical details, let's proceed to understand the concept and process of precisely how the model operates.</p></section><div class="last-modified">Last modified: 22 February 2024</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="introduction.html" class="navigation-links__prev">Background</a><a href="how-llm-works.html" class="navigation-links__next">How LLM Works</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b205/app.js"></script></body></html>