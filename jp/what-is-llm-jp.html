<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-colors-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="built-on" content="2024-02-22T15:49:55.129215"><title>LLMとは | LLM：ゼロからエキスパートへ</title><script type="application/json" id="virtual-toc-data">[{"id":"85d5f8de_2","level":0,"title":"モデルの定義","anchor":"#85d5f8de_2"},{"id":"model_1","level":0,"title":"Modelが具体的にどのようなものか","anchor":"#model_1"},{"id":"85d5f8de_28","level":0,"title":"モデルの推論","anchor":"#85d5f8de_28"},{"id":"model","level":0,"title":"Modelアーキテクチャ","anchor":"#model"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b205/app.css" rel="stylesheet"><link rel="icon" type="image/png" sizes="16x16" href="images/cropped_logo.png"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="LLMとは | LLM：ゼロからエキスパートへ"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="LLM：ゼロからエキスパートへ Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="/japanese/what-is-llm-jp.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="LLMとは | LLM：ゼロからエキスパートへ"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "/japanese/what-is-llm-jp.html#webpage",
    "url": "/japanese/what-is-llm-jp.html",
    "name": "LLMとは | LLM：ゼロからエキスパートへ",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "/japanese/#website",
    "url": "/japanese/",
    "name": "LLM：ゼロからエキスパートへ Help"
}</script><!-- End Schema.org --></head><body data-id="What-is-LLM-JP" data-main-title="LLMとは" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs=""><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>LLM：ゼロからエキスパートへ  Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="What-is-LLM-JP" id="What-is-LLM-JP.md">LLMとは</h1><section class="chapter"><h2 id="85d5f8de_2" data-toc="85d5f8de_2">モデルの定義</h2><p id="85d5f8de_3">技術業界では、ChatGPTのような大規模言語モデル（LLM）が注目を集めています。 [Wikipedia] によると、LLMの定義は以下のようになっています。</p><p id="85d5f8de_4">大規模言語モデル（LLM）は、一般的な言語理解と生成の能力が特徴の言語モデルである。</p><p id="85d5f8de_5">LLMは、計算量の多い自己教師ありおよび半教師ありのトレーニングプロセス中に、 テキスト文書から統計的関係を学習することによってこれらの能力を獲得します。</p><p id="85d5f8de_6">LLMは、トランスフォーマーアーキテクチャに従う人工ニューラルネットワークです。 言い換えると、 LLMは、本やウェブサイト、ユーザーが生成したコンテンツなどの大規模なテキストデータセットでトレーニングされます。</p><p id="85d5f8de_7">これにより、自然な方法で初期プロンプトを継続する新しいテキストを生成できます。</p><p id="85d5f8de_8">LLM Modelは、基本的に多くのパラメータを持つニューラルネットワークです。</p><p id="85d5f8de_9">大まかに言えば、パラメータが多いほど、Modelは優れています。</p><p id="85d5f8de_10">私たちは常にModelのサイズ、つまりパラメータの数がよく話題になります。</p><p id="85d5f8de_11">例えば、GPT-3は1750億のパラメータを持ち、GPT-4は1兆を超えるパラメータがあると言われています。</p></section><section class="chapter"><h2 id="model_1" data-toc="model_1">Modelが具体的にどのようなものか</h2><p id="85d5f8de_12">言語Modelはバイナリファイルです。</p><figure id="85d5f8de_13"><img alt="A language model is just a file" src="images/a-language-model-is-just-a-file.png" title="A language model is just a file" width="400" height="465"></figure><p id="85d5f8de_14">(from <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" id="85d5f8de_15" data-external="true" rel="noopener noreferrer">Andrej Karpathy's build a GPT model from scratch)</a>)</p><p id="85d5f8de_16">上の画像では、パラメータファイルはMetaのLlama-2-70bModelで、サイズは140GBで、70bのパラメータ（数字の形式）を含んでいます。</p><p id="85d5f8de_17">run.cファイルは推論プログラムで、Modelを問い合わせるための使用されます。</p><p id="85d5f8de_18">これらの超大規模Modelをトレーニングするのは非常に高価です。</p><p id="85d5f8de_19">GPT-3のようなModelをトレーニングするのに数百万ドルかかります。</p><figure id="85d5f8de_20"><img alt="Train on gpu concept" src="images/train-on-gpu-concept.png" title="Train on gpu concept" width="600" height="303"></figure><p id="85d5f8de_21">(from <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" id="85d5f8de_22" data-external="true" rel="noopener noreferrer">Andrej Karpathy's build a GPT model from scratch)</a>)</p><p id="85d5f8de_23">現在、最も優れたModelであるGPT-4は、単一のModelではなく、複数のModelの混合物です。</p><p id="85d5f8de_24">各Modelは、特定のドメインでトレーニングまたはファインチューニングされ、 推論中に最高のパフォーマンスを発揮するために協力して動作します。</p><p id="85d5f8de_25">私たちの目標は、大規模言語Modelの基礎を理解し、学ぶことです。</p><p id="85d5f8de_26">幸いなことに、個人のコンピュータでははるかに少ないパラメータを持つModelをトレーニングすることができます。</p><p id="85d5f8de_27">[Let's Code LLM] セクションで、一緒にステップバイステップでコードを書いていきましょう。</p></section><section class="chapter"><h2 id="85d5f8de_28" data-toc="85d5f8de_28">モデルの推論</h2><p id="85d5f8de_29">Modelがトレーニングされて準備が整ったら、ユーザーはModelに質問を投げかけます。</p><p id="85d5f8de_30">質問テキストは、140GBのファイルに渡され、文字ごとに処理され後、 最も関連性の高いテキストとして結果が返されます。</p><p id="85d5f8de_31">最も関連性の高いという意味は、 Modelが入力テキストの次の文字として最もありそうなテキストを返すという意味です。</p><p id="85d5f8de_32">例えば</p><div class="code-block" data-lang="none">
&gt; Input: &quot;I like to eat&quot;

&gt; Output: &quot;apple&quot;

</div><p id="85d5f8de_34">&quot;apple&quot;は次の文字として返されます。</p><p id="85d5f8de_35">なぜなら、&quot;I like to eat&quot;の後に&quot;apple&quot;が最もありそうな文字だからです。</p><p id="85d5f8de_36">これは、Modelがトレーニングされた大規模なデータセットに基づいています。</p><p id="85d5f8de_37">上記で言及された本やウェブサイトを基に考えると、 このように理解できます。</p><p id="85d5f8de_38">&quot;I like to eat apple&quot;は、Modelが複数回&quot;学習&quot;した非常に一般的な文です。 &quot;I like to eat banana&quot;も一般的な文ですが、上記の文よりは一般的ではありません。</p><p id="85d5f8de_39">トレーニング中、Modelは次のように記録します。</p><p id="85d5f8de_40">&quot;I like to eat&quot;の後に&quot;apple&quot;が<span class="control" id="85d5f8de_41">0.375</span>の確率で現れることを記録しました。 &quot;I like to eat&quot;の後に&quot;banana&quot;が<span class="control" id="85d5f8de_42">0.146</span>の確率で現れることを記録しました。</p><p id="85d5f8de_43">これらの確率は、 <span class="control" id="85d5f8de_44">パラメータ</span>Modelファイルに保存された<span class="control" id="85d5f8de_45">確率セット</span>として保存されます。 (確率は機械学習分野では<span class="control" id="85d5f8de_46">重み</span>と呼ばれることが多いです。)</p><p id="85d5f8de_47">つまり基本的に、LLM Modelは、与えられた文字とその関連するコンテキスト文字に確率分布を割り当てる確率的データベースです。</p><p id="85d5f8de_48">これは以前には不可能に思えたかもしれません。しかし、2017年に <a href="https://arxiv.org/abs/2005.14165" id="85d5f8de_49" data-external="true" rel="noopener noreferrer">《Attention is all you need》</a> という論文が発表されて以来、 非常に大規模なデータセットでニューラルネットワークをトレーニングすることにより、このような文脈理解を可能にする<span class="control" id="85d5f8de_50">transformer architecture</span>が導入されました。</p></section><section class="chapter"><h2 id="model" data-toc="model">Modelアーキテクチャ</h2><p id="85d5f8de_51">LLMの登場以前、ニューラルネットワークの機械学習は比較的小規模なデータセットで使用し、文脈理解の能力は限られていました。</p><p id="85d5f8de_52">これは、初期もModelが人間のようにテキストを理解する能力を持っていなかったためです。</p><p id="85d5f8de_53">論文が発表されたとき、それは言語翻訳を目的としたModelの訓練に向けられていました。</p><p id="85d5f8de_54">OpenAIのチームは、トランスフォーマーアーキテクチャが文字予測のための決定的な解決策であることを発見しました。</p><p id="85d5f8de_55">インターネットデータ全体でトレーニングされると、Modelは任意のテキストを文脈を理解し、 人間のように文章を緻密に完成されることができるようになりました。</p><p id="85d5f8de_56">以下の図は、Modelのトレーニングプロセス内部で何が起こっているかを示しています。</p><figure id="85d5f8de_57"><img alt="Llm diagram1" src="images/llm-diagram1.png" title="Llm diagram1" width="800" height="411"></figure><p id="85d5f8de_58">もちろん、私たちは最初にそれを見た時にすぐに理解することはできませんが、 これから、以下のトピックでその内容を詳しく見て行きます。</p><p id="85d5f8de_59">プログラム的な観点や数学てな詳細に入る前に、 Modelがどのように動作するかの概念とプロセスを正確に理解するために進めましょう。</p></section><div class="last-modified">Last modified: 22 February 2024</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="introduction-jp.html" class="navigation-links__prev">自己紹介</a><a href="how-llm-works-jp.html" class="navigation-links__next">大規模言語モデルについて</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.6.6-b205/app.js"></script></body></html>